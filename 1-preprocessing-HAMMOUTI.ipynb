{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKHYeMT7jfDG"
   },
   "source": [
    "## **HAMMOUTI Douae**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l14h_1NjfDJ"
   },
   "source": [
    "The first part of this notebook is based on Sudalai Rajkumar's tutorial on Kaggle.\n",
    "More information on the [***dataset***](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter).\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important.\n",
    "\n",
    "The goal of this tutorial is to understand the various text preprocessing steps with code examples.\n",
    "\n",
    "Some of the common text preprocessing / cleaning steps are:\n",
    "* Lower casing\n",
    "* Removal of Punctuations\n",
    "* Removal of Stopwords\n",
    "* Removal of Frequent words\n",
    "* Removal of Rare words\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Removal of emojis\n",
    "* Removal of emoticons\n",
    "* Conversion of emoticons to words\n",
    "* Conversion of emojis to words\n",
    "* Removal of URLs\n",
    "* Removal of HTML tags\n",
    "* Chat words conversion\n",
    "* Spelling correction\n",
    "\n",
    "\n",
    "So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role.\n",
    "\n",
    "For example, in sentiment analysis use case, we need not remove the emojis or emoticons as they will convey some important information about the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "YZaU1CbdjfDL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdKCHXKzjfDM"
   },
   "source": [
    "Let's load the dataset and see how is structured with few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TgZK-l3CjfDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (93, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119237</td>\n",
       "      <td>105834</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 06:55:44 +0000 2017</td>\n",
       "      <td>@AppleSupport causing the reply to be disregar...</td>\n",
       "      <td>119236</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119238</td>\n",
       "      <td>ChaseSupport</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 11 13:25:49 +0000 2017</td>\n",
       "      <td>@105835 Your business means a lot to us. Pleas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119239</td>\n",
       "      <td>105835</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Oct 11 13:00:09 +0000 2017</td>\n",
       "      <td>@76328 I really hope you all change but I'm su...</td>\n",
       "      <td>119238</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119240</td>\n",
       "      <td>VirginTrains</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 10 15:16:08 +0000 2017</td>\n",
       "      <td>@105836 LiveChat is online at the moment - htt...</td>\n",
       "      <td>119241</td>\n",
       "      <td>119242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119241</td>\n",
       "      <td>105836</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 10 15:17:21 +0000 2017</td>\n",
       "      <td>@VirginTrains see attached error message. I've...</td>\n",
       "      <td>119243</td>\n",
       "      <td>119240.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id     author_id  inbound                      created_at  \\\n",
       "0    119237        105834     True  Wed Oct 11 06:55:44 +0000 2017   \n",
       "1    119238  ChaseSupport    False  Wed Oct 11 13:25:49 +0000 2017   \n",
       "2    119239        105835     True  Wed Oct 11 13:00:09 +0000 2017   \n",
       "3    119240  VirginTrains    False  Tue Oct 10 15:16:08 +0000 2017   \n",
       "4    119241        105836     True  Tue Oct 10 15:17:21 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @AppleSupport causing the reply to be disregar...            119236   \n",
       "1  @105835 Your business means a lot to us. Pleas...               NaN   \n",
       "2  @76328 I really hope you all change but I'm su...            119238   \n",
       "3  @105836 LiveChat is online at the moment - htt...            119241   \n",
       "4  @VirginTrains see attached error message. I've...            119243   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      NaN  \n",
       "1                 119239.0  \n",
       "2                      NaN  \n",
       "3                 119242.0  \n",
       "4                 119240.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/tweets_preprocessing.csv\")\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "id": "DB6L8_qbjfDN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## **Lower Casing**\n",
    "\n",
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n",
    "\n",
    "This is more helpful when computing words frequency for example, yet it may not be the case for tasks like Part of Speech tagging (where proper casing gives some information about proper nouns and so on) and Sentiment Analysis (where upper casing refers to anger)\n",
    "\n",
    "By default, lower casing is done my most of the modern day vectorizers and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cswaTT3LjfDO"
   },
   "outputs": [],
   "source": [
    "text_df = df[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dFjNCwB6jfDO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lowering:\n",
      "['@AppleSupport causing the reply to be disregarded and the tapped notification under the keyboard is openedðŸ˜¡ðŸ˜¡ðŸ˜¡'\n",
      " '@105835 Your business means a lot to us. Please DM your name, zip code and additional details about your concern. ^RR https://t.co/znUu1VJn9r'\n",
      " \"@76328 I really hope you all change but I'm sure you won't! Because you don't have to!\"\n",
      " '@105836 LiveChat is online at the moment - https://t.co/SY94VtU8Kq or contact 03331 031 031 option 1, 4, 3 (Leave a message) to request a call back'\n",
      " \"@VirginTrains see attached error message. I've tried leaving a voicemail several times in the past week https://t.co/NxVZjlYx1k\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before lowering:\")\n",
    "print(text_df.head().text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aw1lyIskjfDP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lowering:\n",
      "['@applesupport causing the reply to be disregarded and the tapped notification under the keyboard is openedðŸ˜¡ðŸ˜¡ðŸ˜¡'\n",
      " '@105835 your business means a lot to us. please dm your name, zip code and additional details about your concern. ^rr https://t.co/znuu1vjn9r'\n",
      " \"@76328 i really hope you all change but i'm sure you won't! because you don't have to!\"\n",
      " '@105836 livechat is online at the moment - https://t.co/sy94vtu8kq or contact 03331 031 031 option 1, 4, 3 (leave a message) to request a call back'\n",
      " \"@virgintrains see attached error message. i've tried leaving a voicemail several times in the past week https://t.co/nxvzjlyx1k\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @applesupport causing the reply to be disregar...\n",
       "1  @105835 your business means a lot to us. pleas...\n",
       "2  @76328 i really hope you all change but i'm su...\n",
       "3  @105836 livechat is online at the moment - htt...\n",
       "4  @virgintrains see attached error message. i've..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "# convert the text_df text column to lowercase\n",
    "# keep in mind that text_df[\"text\"] is a Pandas' Series\n",
    "text_df[\"text\"] = text_df[\"text\"].str.lower()\n",
    "\n",
    "\n",
    "assert text_df.text.values[2] == \"@76328 i really hope you all change but i'm sure you won't! because you don't have to!\", \"Your text is not lowered properly\"\n",
    "\n",
    "print(\"After lowering:\")\n",
    "print(text_df.head().text.values)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8edjLWCjfDQ"
   },
   "source": [
    "## **Removal of Punctuations**\n",
    "\n",
    "Another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
    "\n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the `string.punctuation` in python contains the following punctuation symbols\n",
    "\n",
    "`!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~`\n",
    "\n",
    "We can add or remove more punctuations as per our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yivrRRvAjfDR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sdjRzwgujfDR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \n",
       "0  applesupport causing the reply to be disregard...  \n",
       "1  105835 your business means a lot to us please ...  \n",
       "2  76328 i really hope you all change but im sure...  \n",
       "3  105836 livechat is online at the moment  https...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation characters from the input text\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which punctuation characters will be removed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all punctuation characters removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # use the maketrans function to remove the punctuation specified in PUNCT_TO_REMOVE [https://www.w3schools.com/python/ref_string_maketrans.asp]\n",
    "    text_df= str.maketrans(\"\",\"\",PUNCT_TO_REMOVE)\n",
    "    return text.translate(text_df)\n",
    "\n",
    "# now apply your function to the text column using the apply function [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html]\n",
    "text_df[\"text_wo_punct\"] = text_df[\"text\"].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "\n",
    "assert text_df[\"text_wo_punct\"].values[3] == \"105836 livechat is online at the moment  httpstcosy94vtu8kq or contact 03331 031 031 option 1 4 3 leave a message to request a call back\"\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJynQ8KKjfDS"
   },
   "source": [
    "## **Removal of stopwords**\n",
    "\n",
    "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
    "\n",
    "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GP9M8NEbjfDS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can, couldn, couldn't, d, did, didn, didn't, do, does, doesn, doesn't, doing, don, don't, down, during, each, few, for, from, further, had, hadn, hadn't, has, hasn, hasn't, have, haven, haven't, having, he, he'd, he'll, her, here, hers, herself, he's, him, himself, his, how, i, i'd, if, i'll, i'm, in, into, is, isn, isn't, it, it'd, it'll, it's, its, itself, i've, just, ll, m, ma, me, mightn, mightn't, more, most, mustn, mustn't, my, myself, needn, needn't, no, nor, not, now, o, of, off, on, once, only, or, other, our, ours, ourselves, out, over, own, re, s, same, shan, shan't, she, she'd, she'll, she's, should, shouldn, shouldn't, should've, so, some, such, t, than, that, that'll, the, their, theirs, them, themselves, then, there, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, ve, very, was, wasn, wasn't, we, we'd, we'll, we're, were, weren, weren't, we've, what, when, where, which, while, who, whom, why, will, with, won, won't, wouldn, wouldn't, y, you, you'd, you'll, your, you're, yours, yourself, yourselves, you've\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn62G30DjfDS"
   },
   "source": [
    "Similarly we can also get the list for other languages as well and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iaQN0vdvjfDT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@applesupport causing the reply to be disregarded and the tapped notification under the keyboard is openedðŸ˜¡ðŸ˜¡ðŸ˜¡'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text_df.text.values[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "24qqm8GwjfDT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@applesupport',\n",
       " 'causing',\n",
       " 'the',\n",
       " 'reply',\n",
       " 'to',\n",
       " 'be',\n",
       " 'disregarded',\n",
       " 'and',\n",
       " 'the',\n",
       " 'tapped',\n",
       " 'notification',\n",
       " 'under',\n",
       " 'the',\n",
       " 'keyboard',\n",
       " 'is',\n",
       " 'openedðŸ˜¡ðŸ˜¡ðŸ˜¡']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = sample.split(' ') # splits the input string into a list, the delimiter is a whitespace \" \"\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2Tm_tlcQjfDT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@applesupport',\n",
       " 'causing',\n",
       " 'reply',\n",
       " 'disregarded',\n",
       " 'tapped',\n",
       " 'notification',\n",
       " 'keyboard',\n",
       " 'openedðŸ˜¡ðŸ˜¡ðŸ˜¡']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use list comprehension to remove the stopwords: [https://www.programiz.com/python-programming/list-comprehension]\n",
    "# We want to achieve the same result as:\n",
    "\n",
    "# filtered_words = []\n",
    "# for word in split:\n",
    "#   if word not in STOPWORDS:\n",
    "#       filtered_words.append(word)\n",
    "#\n",
    "# Your code here\n",
    "filtered_words = [word for word in split if word not in STOPWORDS]\n",
    "\n",
    "\n",
    "assert filtered_words == ['@applesupport','causing','reply','disregarded','tapped','notification','keyboard','openedðŸ˜¡ðŸ˜¡ðŸ˜¡']\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "12iKkg6bjfDT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: @applesupport causing the reply to be disregarded and the tapped notification under the keyboard is openedðŸ˜¡ðŸ˜¡ðŸ˜¡\n",
      "After filtering: @applesupport causing reply disregarded tapped notification keyboard openedðŸ˜¡ðŸ˜¡ðŸ˜¡\n"
     ]
    }
   ],
   "source": [
    "filtered_string = \" \".join(filtered_words)\n",
    "print(f\"Before filtering: {sample}\\nAfter filtering: {filtered_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kIPBkaZJjfDU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "      <td>applesupport causing reply disregarded tapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "      <td>105835 business means lot us please dm name zi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0  applesupport causing the reply to be disregard...   \n",
       "1  105835 your business means a lot to us please ...   \n",
       "2  76328 i really hope you all change but im sure...   \n",
       "3  105836 livechat is online at the moment  https...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                        text_wo_stop  \n",
       "0  applesupport causing reply disregarded tapped ...  \n",
       "1  105835 business means lot us please dm name zi...  \n",
       "2         76328 really hope change im sure wont dont  \n",
       "3  105836 livechat online moment httpstcosy94vtu8...  \n",
       "4  virgintrains see attached error message ive tr...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which stopwords will be removed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string without stopwords\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # remove stopwords with list comprehension and convert the list back to a string by concatenating the words\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in STOPWORDS]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "text_df[\"text_wo_stop\"] = text_df[\"text_wo_punct\"].apply(remove_stopwords)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOWVEknLjfDU"
   },
   "source": [
    "## **Removal of Frequent words**\n",
    "\n",
    "In the previous preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of lesser importance to us.\n",
    "\n",
    "So this step is to remove the frequent words in the given corpus. So, let us get the most common words and then remove them in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ex7jSaPbjfDU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('us', 25),\n",
       " ('dm', 19),\n",
       " ('help', 18),\n",
       " ('thanks', 13),\n",
       " ('httpstcogdrqu22ypt', 12),\n",
       " ('applesupport', 11),\n",
       " ('please', 11),\n",
       " ('phone', 9),\n",
       " ('hi', 9),\n",
       " ('ive', 8)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "\n",
    "# Your code here:\n",
    "# Use the Counter class to return the most frequent words\n",
    "# 1: join all the text in the text_wo_stop column using the join() function\n",
    "big_string = \" \".join(text_df[\"text_wo_stop\"])\n",
    "# 2: tokenize the text on white space by using the split() function\n",
    "tokens = big_string.split()\n",
    "# 3: instantiate the Counter class with your tokenized array\n",
    "cnt = Counter(tokens)\n",
    "# 4: use the most_common class method to return the most frequent words\n",
    "most_common = cnt.most_common()\n",
    "\n",
    "assert set(most_common[:10]) == set([('us', 25), ('dm', 19),('help', 18),('thanks', 13),('httpstcogdrqu22ypt',12),('applesupport', 11),('please', 11),('phone', 9),('hi', 9),('ive', 8)])\n",
    "most_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "c-5wAvL8jfDU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "      <th>text_wo_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>applesupport causing the reply to be disregard...</td>\n",
       "      <td>applesupport causing reply disregarded tapped ...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 your business means a lot to us please ...</td>\n",
       "      <td>105835 business means lot us please dm name zi...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 i really hope you all change but im sure...</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat is online at the moment  https...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message ive tr...</td>\n",
       "      <td>virgintrains see attached error message tried ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0  applesupport causing the reply to be disregard...   \n",
       "1  105835 your business means a lot to us please ...   \n",
       "2  76328 i really hope you all change but im sure...   \n",
       "3  105836 livechat is online at the moment  https...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                        text_wo_stop  \\\n",
       "0  applesupport causing reply disregarded tapped ...   \n",
       "1  105835 business means lot us please dm name zi...   \n",
       "2         76328 really hope change im sure wont dont   \n",
       "3  105836 livechat online moment httpstcosy94vtu8...   \n",
       "4  virgintrains see attached error message ive tr...   \n",
       "\n",
       "                                        text_wo_freq  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2         76328 really hope change im sure wont dont  \n",
       "3  105836 livechat online moment httpstcosy94vtu8...  \n",
       "4  virgintrains see attached error message tried ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set with the 10 most frequent words:\n",
    "# hint: create a set comprehension equivalent to\n",
    "# words = set()\n",
    "# for word, count in most_common[:10]:\n",
    "#   words.add(word)\n",
    "#\n",
    "# keep in mind that sets are more efficient in this scenario, being implemented as hash tables\n",
    "\n",
    "FREQWORDS = {w for (w, word_count) in most_common[:10]}\n",
    "\n",
    "def remove_freqwords(text: str, freq_words: set=FREQWORDS) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of frequent words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which frequent words will be removed\n",
    "        freq_words (set): A set of frequent words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all frequent words removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # use your function to filter out the 10 most frequent words\n",
    "    return \" \".join([word for word in text.split() if word not in freq_words])\n",
    "\n",
    "\n",
    "assert remove_freqwords(text_df.text_wo_stop.values[1]) == \"105835 business means lot name zip code additional details concern rr httpstcoznuu1vjn9r\"\n",
    "\n",
    "# Your code here:\n",
    "# Apply your function to the text_wo_stop column\n",
    "text_df[\"text_wo_freq\"] = text_df[\"text_wo_stop\"].apply(remove_freqwords)\n",
    "\n",
    "\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B95U0CSEjfDU"
   },
   "source": [
    "## **Removal of Rare words**\n",
    "\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QiyQvpJbjfDV",
    "outputId": "49fcacd1-9cbd-4399-b1bf-a554cd0149e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message tried ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                        text_wo_freq  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2         76328 really hope change im sure wont dont  \n",
       "3  105836 livechat online moment httpstcosy94vtu8...  \n",
       "4  virgintrains see attached error message tried ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's keep only the latest version of the processed text\n",
    "text_df = text_df[[\"text\", \"text_wo_freq\"]]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_cjDUpEBjfDV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'browser',\n",
       " 'green',\n",
       " 'httpstco9281okeebk',\n",
       " 'including',\n",
       " 'keen',\n",
       " 'lee',\n",
       " 'line',\n",
       " 'log',\n",
       " 'slowdown',\n",
       " 'thin'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "# similarly to FREQWORD, extract the 10 rarest words\n",
    "RAREWORDS = {w for (w, word_count) in most_common[-10:]}\n",
    "\n",
    "\n",
    "assert RAREWORDS == set(['browser', 'green', 'httpstco9281okeebk', 'including', 'keen', 'lee', 'line', 'log','slowdown','thin'])\n",
    "RAREWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TTPWQTKJjfDV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_freq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "      <td>causing reply disregarded tapped notification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "      <td>105835 business means lot name zip code additi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "      <td>76328 really hope change im sure wont dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "      <td>105836 livechat online moment httpstcosy94vtu8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>virgintrains see attached error message tried ...</td>\n",
       "      <td>virgintrains see attached error message tried ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                        text_wo_freq  \\\n",
       "0  causing reply disregarded tapped notification ...   \n",
       "1  105835 business means lot name zip code additi...   \n",
       "2         76328 really hope change im sure wont dont   \n",
       "3  105836 livechat online moment httpstcosy94vtu8...   \n",
       "4  virgintrains see attached error message tried ...   \n",
       "\n",
       "                                text_wo_stopfreqrare  \n",
       "0  causing reply disregarded tapped notification ...  \n",
       "1  105835 business means lot name zip code additi...  \n",
       "2         76328 really hope change im sure wont dont  \n",
       "3  105836 livechat online moment httpstcosy94vtu8...  \n",
       "4  virgintrains see attached error message tried ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_rarewords(text: str, rare_words: set=RAREWORDS) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of rare words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which rare words will be removed\n",
    "        rare_words (set): A set of rare words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all rare words removed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # Filter out the most frequent words from a text string\n",
    "    return \" \".join([word for word in text.split() if word not in rare_words])\n",
    "\n",
    "\n",
    "text_df[\"text_wo_stopfreqrare\"] = text_df[\"text_wo_freq\"].apply(remove_rarewords)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emcdpv7tjfDV"
   },
   "source": [
    "We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "d3sVXAKEjfDV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'applesupport',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'browser',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'dm',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'green',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'httpstco9281okeebk',\n",
       " 'httpstcogdrqu22ypt',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'including',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " 'just',\n",
       " 'keen',\n",
       " 'lee',\n",
       " 'line',\n",
       " 'll',\n",
       " 'log',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'phone',\n",
       " 'please',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'slowdown',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'thanks',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thin',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'us',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "# group all the words to remove using in a single set (TO_REMOVE)\n",
    "TO_REMOVE = STOPWORDS.union(FREQWORDS).union(RAREWORDS)\n",
    "\n",
    "TO_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FDc4NW1-jfDV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing reply disregarded tapped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 business means lot us. name, zip code ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 really hope change sure won't! to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat online moment - https://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. trie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \n",
       "0  @applesupport causing reply disregarded tapped...  \n",
       "1  @105835 business means lot us. name, zip code ...  \n",
       "2          @76328 really hope change sure won't! to!  \n",
       "3  @105836 livechat online moment - https://t.co/...  \n",
       "4  @virgintrains see attached error message. trie...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_text(text: str, words_to_remove :set=TO_REMOVE) -> str:\n",
    "    \"\"\"\n",
    "    Removes a selection of words from the input string\n",
    "\n",
    "    Inputs:\n",
    "        text (str): The input text from which words will be removed\n",
    "        words_to_remove (set): A set of words to remove from the text\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with all listed words removed\n",
    "    \"\"\" \n",
    "    # Your code here\n",
    "    return \" \".join([word for word in text.split() if word not in words_to_remove])\n",
    "\n",
    "\n",
    "text_df[\"filtered_text\"] =  text_df.text.apply(filter_text)\n",
    "text_df = text_df[['text', 'filtered_text']]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsk2Df6hjfDV"
   },
   "source": [
    "## **Stemming**\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https://en.wikipedia.org/wiki/Stemming)).\n",
    "\n",
    "This process is useful to **`reduce the vocabulary size`** by converting similar words to their root form.\n",
    "\n",
    "For example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n",
    "\n",
    "There are several type of stemming algorithms available and one of the famous one is porter stemmer (NLTK package) which is widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wb2ycjl-jfDV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: programs, \tafter stemming: program\n",
      "Before stemming: programming, \tafter stemming: program\n",
      "Before stemming: programmed, \tafter stemming: program\n",
      "Before stemming: walks, \tafter stemming: walk\n",
      "Before stemming: walked, \tafter stemming: walk\n",
      "Before stemming: walking, \tafter stemming: walk\n",
      "Before stemming: UPPERCASE, \tafter stemming: uppercas\n",
      "Before stemming: mistake, \tafter stemming: mistak\n",
      "Before stemming: mistakke, \tafter stemming: mistakk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_demonstration(word: str) -> None:\n",
    "    \"\"\"\n",
    "    Compares a word before and after stemming\n",
    "    \"\"\"\n",
    "    print(f\"Before stemming: {word}, \\tafter stemming: {stemmer.stem(word)}\")\n",
    "\n",
    "for word in ['programs', 'programming', 'programmed', 'walks', 'walked', 'walking', 'UPPERCASE', 'mistake', 'mistakke']:\n",
    "    stem_demonstration(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7RChbBWZjfDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'@105836 working ok here, miriam. link https://t.co/0m2mph15eh ? ^mm'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing reply disregarded tapped...</td>\n",
       "      <td>@applesupport caus repli disregard tap notif k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 business means lot us. name, zip code ...</td>\n",
       "      <td>@105835 busi mean lot us. name, zip code addit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 really hope change sure won't! to!</td>\n",
       "      <td>@76328 realli hope chang sure won't! to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat online moment - https://t.co/...</td>\n",
       "      <td>@105836 livechat onlin moment - https://t.co/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. trie...</td>\n",
       "      <td>@virgintrain see attach error message. tri lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@105836 have you tried from another device, mi...</td>\n",
       "      <td>@105836 tried another device, miriam ^mm</td>\n",
       "      <td>@105836 tri anoth device, miriam ^mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@virgintrains yep, i've tried laptop too sever...</td>\n",
       "      <td>@virgintrains yep, tried laptop several times ...</td>\n",
       "      <td>@virgintrain yep, tri laptop sever time past w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@105836 it's working ok from here, miriam. doe...</td>\n",
       "      <td>@105836 working ok here, miriam. link https://...</td>\n",
       "      <td>@105836 work ok here, miriam. link https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virgintrains i still haven't heard &amp;amp; the ...</td>\n",
       "      <td>@virgintrains still heard &amp;amp; number directe...</td>\n",
       "      <td>@virgintrain still heard &amp;amp; number direct d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@105836 that's what we're here for miriam ðŸ˜Š  t...</td>\n",
       "      <td>@105836 that's miriam ðŸ˜Š team send email shortl...</td>\n",
       "      <td>@105836 that' miriam ðŸ˜Š team send email shortli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "5  @105836 have you tried from another device, mi...   \n",
       "6  @virgintrains yep, i've tried laptop too sever...   \n",
       "7  @105836 it's working ok from here, miriam. doe...   \n",
       "8  @virgintrains i still haven't heard &amp; the ...   \n",
       "9  @105836 that's what we're here for miriam ðŸ˜Š  t...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  @applesupport causing reply disregarded tapped...   \n",
       "1  @105835 business means lot us. name, zip code ...   \n",
       "2          @76328 really hope change sure won't! to!   \n",
       "3  @105836 livechat online moment - https://t.co/...   \n",
       "4  @virgintrains see attached error message. trie...   \n",
       "5           @105836 tried another device, miriam ^mm   \n",
       "6  @virgintrains yep, tried laptop several times ...   \n",
       "7  @105836 working ok here, miriam. link https://...   \n",
       "8  @virgintrains still heard &amp; number directe...   \n",
       "9  @105836 that's miriam ðŸ˜Š team send email shortl...   \n",
       "\n",
       "                                        text_stemmed  \n",
       "0  @applesupport caus repli disregard tap notif k...  \n",
       "1  @105835 busi mean lot us. name, zip code addit...  \n",
       "2           @76328 realli hope chang sure won't! to!  \n",
       "3  @105836 livechat onlin moment - https://t.co/s...  \n",
       "4  @virgintrain see attach error message. tri lea...  \n",
       "5               @105836 tri anoth device, miriam ^mm  \n",
       "6  @virgintrain yep, tri laptop sever time past w...  \n",
       "7  @105836 work ok here, miriam. link https://t.c...  \n",
       "8  @virgintrain still heard &amp; number direct d...  \n",
       "9  @105836 that' miriam ðŸ˜Š team send email shortli...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies the stemmer to an input string\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be stemmed\n",
    "\n",
    "    Returns:\n",
    "        str: A new string where every word has been stemmed\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in text.split()])\n",
    "\n",
    "print(repr(text_df[\"filtered_text\"].values[7]))\n",
    "assert stem_words(text_df[\"filtered_text\"].values[7]) == \"@105836 work ok here, miriam. link https://t.co/0m2mph15eh ? ^mm\" # note that the stemmer also applied lowercase\n",
    "\n",
    "text_df[\"text_stemmed\"] = text_df[\"filtered_text\"].apply(stem_words)\n",
    "text_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hpxjYRWQjfDX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words without stemming: 813\n",
      "Number of unique words with stemming: 657\n",
      "Difference: 156 words\n"
     ]
    }
   ],
   "source": [
    "all_text_no_stemming = ' '.join(text_df[\"text\"]).split()\n",
    "all_text_w_stemming = ' '.join(text_df[\"text_stemmed\"]).split()\n",
    "\n",
    "n_words_no_stemming = len(set(all_text_no_stemming))\n",
    "n_words_w_stemming = len(set(all_text_w_stemming))\n",
    "vocabulary_size_diff = n_words_no_stemming - n_words_w_stemming\n",
    "\n",
    "assert vocabulary_size_diff == 156\n",
    "\n",
    "print(f\"Number of unique words without stemming: {n_words_no_stemming}\")\n",
    "print(f\"Number of unique words with stemming: {n_words_w_stemming}\")\n",
    "print(f\"Difference: {vocabulary_size_diff} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G8yK65DjfDX"
   },
   "source": [
    "We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do for that? We can use Lemmatization in such cases.\n",
    "\n",
    "This porter stemmer is for English language only, if we are working with other languages, we can use the snowball stemmer. The supported languages for snowball stemmer are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "600sKQCIjfDX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pTCJiRQjfDX"
   },
   "source": [
    "## **Lemmatization**\n",
    "\n",
    "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n",
    "Here's a list of [examples](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt).\n",
    "\n",
    "As a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\n",
    "\n",
    "Let us use the `WordNetLemmatizer` in nltk to lemmatize our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "I-ZpwJaLjfDX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sqAmxm_mjfDX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: feet\tafter lemmatization: foot\n",
      "Before lemmatization: caring\tafter lemmatization: care\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lem_demonstration(word: str, pos: str='n') -> None:\n",
    "    print(f\"Before lemmatization: {word}\\tafter lemmatization: {lemmatizer.lemmatize(word, pos)}\")\n",
    "\n",
    "for word, pos in [('feet', 'n'), ('caring', 'v')]:\n",
    "    lem_demonstration(word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0fyXubuEjfDY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing reply disregarded tapped...</td>\n",
       "      <td>@applesupport caus repli disregard tap notif k...</td>\n",
       "      <td>@applesupport causing reply disregarded tapped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 business means lot us. name, zip code ...</td>\n",
       "      <td>@105835 busi mean lot us. name, zip code addit...</td>\n",
       "      <td>@105835 business mean lot us. name, zip code a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 really hope change sure won't! to!</td>\n",
       "      <td>@76328 realli hope chang sure won't! to!</td>\n",
       "      <td>@76328 really hope change sure won't! to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat online moment - https://t.co/...</td>\n",
       "      <td>@105836 livechat onlin moment - https://t.co/s...</td>\n",
       "      <td>@105836 livechat online moment - https://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. trie...</td>\n",
       "      <td>@virgintrain see attach error message. tri lea...</td>\n",
       "      <td>@virgintrains see attached error message. trie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  @applesupport causing reply disregarded tapped...   \n",
       "1  @105835 business means lot us. name, zip code ...   \n",
       "2          @76328 really hope change sure won't! to!   \n",
       "3  @105836 livechat online moment - https://t.co/...   \n",
       "4  @virgintrains see attached error message. trie...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  @applesupport caus repli disregard tap notif k...   \n",
       "1  @105835 busi mean lot us. name, zip code addit...   \n",
       "2           @76328 realli hope chang sure won't! to!   \n",
       "3  @105836 livechat onlin moment - https://t.co/s...   \n",
       "4  @virgintrain see attach error message. tri lea...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  @applesupport causing reply disregarded tapped...  \n",
       "1  @105835 business mean lot us. name, zip code a...  \n",
       "2          @76328 really hope change sure won't! to!  \n",
       "3  @105836 livechat online moment - https://t.co/...  \n",
       "4  @virgintrains see attached error message. trie...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies lemmatization to the input string\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized version of the text\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return \" \".join([lemmatizer.lemmatize(word.lower()) for word in text.split()])\n",
    "\n",
    "text_df[\"text_lemmatized\"] = text_df[\"filtered_text\"].apply(lemmatize_words)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYAmNn93jfDY"
   },
   "source": [
    "We can see that the trailing `e` in the `propose` and `private` is retained when we use lemmatization unlike stemming.\n",
    "\n",
    "Wait. There is one more thing in lemmatization. Let us try to lemmatize `running` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "o1T-tA34jfDY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUtAa3U7jfDY"
   },
   "source": [
    "Wow. It returned `running` as such without converting it to the root form `run`. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word.\n",
    "\n",
    "See this [table](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for examples of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ME_hU7mOjfDY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\", \"v\") # v for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hsFa4lWjfDY"
   },
   "source": [
    "Now we are getting the root form `run`. So we also need to provide the POS tag of the word along with the word for lemmatizer in nltk. Depending on the POS, the lemmatizer may return different results.\n",
    "\n",
    "Let us take the example, `stripes` and check the lemma when it is both verb and noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CEnUmW7RjfDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is: stripes\n",
      "Lemma result for verb:  strip\n",
      "Lemma result for noun:  stripe\n"
     ]
    }
   ],
   "source": [
    "print(\"Word is: stripes\")\n",
    "print(\"Lemma result for verb: \",lemmatizer.lemmatize(\"stripes\", 'v'))\n",
    "print(\"Lemma result for noun: \",lemmatizer.lemmatize(\"stripes\", 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rL1Q-OgjfDZ"
   },
   "source": [
    "Now let us redo the lemmatization process for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xA9_SNpwjfDZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MtOLC2XcjfDZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@applesupport causing the reply to be disregar...</td>\n",
       "      <td>@applesupport causing reply disregarded tapped...</td>\n",
       "      <td>@applesupport caus repli disregard tap notif k...</td>\n",
       "      <td>@ applesupport cause reply disregard tapped no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 your business means a lot to us. pleas...</td>\n",
       "      <td>@105835 business means lot us. name, zip code ...</td>\n",
       "      <td>@105835 busi mean lot us. name, zip code addit...</td>\n",
       "      <td>@ 105835 business mean lot u . name , zip code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 i really hope you all change but i'm su...</td>\n",
       "      <td>@76328 really hope change sure won't! to!</td>\n",
       "      <td>@76328 realli hope chang sure won't! to!</td>\n",
       "      <td>@ 76328 really hope change sure wo n't ! to !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 livechat is online at the moment - htt...</td>\n",
       "      <td>@105836 livechat online moment - https://t.co/...</td>\n",
       "      <td>@105836 livechat onlin moment - https://t.co/s...</td>\n",
       "      <td>@ 105836 livechat online moment - http : //t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virgintrains see attached error message. i've...</td>\n",
       "      <td>@virgintrains see attached error message. trie...</td>\n",
       "      <td>@virgintrain see attach error message. tri lea...</td>\n",
       "      <td>@ virgintrains see attach error message . trie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @applesupport causing the reply to be disregar...   \n",
       "1  @105835 your business means a lot to us. pleas...   \n",
       "2  @76328 i really hope you all change but i'm su...   \n",
       "3  @105836 livechat is online at the moment - htt...   \n",
       "4  @virgintrains see attached error message. i've...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  @applesupport causing reply disregarded tapped...   \n",
       "1  @105835 business means lot us. name, zip code ...   \n",
       "2          @76328 really hope change sure won't! to!   \n",
       "3  @105836 livechat online moment - https://t.co/...   \n",
       "4  @virgintrains see attached error message. trie...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  @applesupport caus repli disregard tap notif k...   \n",
       "1  @105835 busi mean lot us. name, zip code addit...   \n",
       "2           @76328 realli hope chang sure won't! to!   \n",
       "3  @105836 livechat onlin moment - https://t.co/s...   \n",
       "4  @virgintrain see attach error message. tri lea...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  @ applesupport cause reply disregard tapped no...  \n",
       "1  @ 105835 business mean lot u . name , zip code...  \n",
       "2      @ 76328 really hope change sure wo n't ! to !  \n",
       "3  @ 105836 livechat online moment - http : //t.c...  \n",
       "4  @ virgintrains see attach error message . trie...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply lemmatization to the input string, considering words' POS tags.\n",
    "\n",
    "    This function lemmatizes words in the input string based on their POS (Part-of-Speech) tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with lemmatized words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a mapping of POS tags to WordNet tags\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Your code here:\n",
    "    # Use the nltk.pos_tag function to get the POS tags of every word in the input (https://www.nltk.org/api/nltk.tag.pos_tag.html)\n",
    "    # You may also use nltk.word_tokenize to tokenize the text (https://www.nltk.org/api/nltk.tokenize.html)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tagged_text = nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "    # Your code here:\n",
    "    # Return the lemmatized version of the text\n",
    "    # Inside the lemmatize function, use the (word, POS tag) tuple collected in the pos_tagged_text list\n",
    "    # hint: query the wordnet_map with wordnet_map.get(key, default) using wordnet.NOUN as default\n",
    "    lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, wordnet_map.get(tag[0], wordnet.NOUN))\n",
    "    for word, tag in pos_tagged_text\n",
    "]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "\n",
    "\n",
    "assert lemmatize_words(\"Any man who must say 'I am the king' is no true king.\") == \"Any man who must say ' I be the king ' be no true king .\"\n",
    "text_df[\"text_lemmatized\"] = text_df[\"filtered_text\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3-opqWrSjfDZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words without stemming: 813\n",
      "Number of unique words with stemming: 592\n",
      "Difference: 221 words out of 93 sample\n"
     ]
    }
   ],
   "source": [
    "all_text_no_lemm = ' '.join(text_df[\"text\"]).split()\n",
    "all_text_w_lemm = ' '.join(text_df[\"text_lemmatized\"]).split()\n",
    "\n",
    "n_words_no_lemm = len(set(all_text_no_lemm))\n",
    "n_words_w_lemm = len(set(all_text_w_lemm))\n",
    "vocabulary_size_diff = n_words_no_lemm - n_words_w_lemm\n",
    "\n",
    "assert vocabulary_size_diff == 221\n",
    "\n",
    "print(f\"Number of unique words without stemming: {n_words_no_lemm}\")\n",
    "print(f\"Number of unique words with stemming: {n_words_w_lemm}\")\n",
    "print(f\"Difference: {vocabulary_size_diff} words out of {df.shape[0]} sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9xESdjjjfDZ"
   },
   "source": [
    "## **Removal of URLs**\n",
    "\n",
    "Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis.\n",
    "\n",
    "We can use the below code snippet to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_Q9LTlnjfDZ"
   },
   "source": [
    "`Regex breakdown:`\n",
    "```Python\n",
    "r'https?://\\S+|www\\.\\S+'\n",
    "# could also be understood as\n",
    "(r'https?://\\S+') or (r'www\\.\\S+')\n",
    "```\n",
    "* `r` in front of a string indicates that Python shall treat the string as a raw litteral (avoids `\\` being treated as escape characters)\n",
    "* `https?://'`: This part of the regular expression matches URLs that start with either \"http://\" or \"https:////\". The `s?` portion allows for an optional \"s\" character, so it matches both \"http://\" and \"https://\".\n",
    "*  `\\S+`: This part of the regular expression matches one or more non-whitespace characters. It's used to match the domain part of the URL (e.g., www.example.com).\n",
    "|: This is the alternation operator, which acts like an OR operator in regular expressions. It allows you to match either the pattern on the left or the pattern on the right. In this case, it's used to match either URLs starting with \"http://\" or \"https://\", or URLs starting with \"www.\".\n",
    "* `www\\.\\S+`: This part of the regular expression matches URLs that start with \"www.\" and then followed by one or more non-whitespace characters. It's commonly used to match URLs like \"www.example.com\".\n",
    "\n",
    "In summary, this regular expression is designed to identify and capture URLs in a text string, whether they start with `\"http://\"`, `\"https://\"`, or `\"www.\"`. It's a common pattern for extracting or hyperlinking URLs in text processing tasks.\n",
    "So, when you see `u'('+emot+')'`, it's creating a Unicode string that contains a left parenthesis `'('`, the value of the `emot` variable (which is a placeholder for the text or pattern you want to find), and a right parenthesis `')'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "B8YxYPEMjfDZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check this out:  and also '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_urls(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs (web links) from the input text.\n",
    "\n",
    "    This function searches for URLs in the input text and removes them, leaving the\n",
    "    text without any web links.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which URLs will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with URLs removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_urls(\"Visit our website at https://www.example.com to learn more.\")\n",
    "        \"Visit our website at to learn more.\"\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Regex pattern for URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    # Replace all URLs with an empty string\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "text = \"Check this out: https://example.com and also www.test.com\"\n",
    "remove_urls(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Gf4ZQKjfDa"
   },
   "source": [
    "Let us take a `https` link and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UxnBgRaljfDa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Driverless AI NLP blog post on '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVFIqXJcjfDa"
   },
   "source": [
    "Now let us take a `http` url and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "N2B0b4YvjfDa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please refer to link  for the paper'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Please refer to link http://lnkd.in/ecnt5yC for the paper\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drDSaV_tjfDa"
   },
   "source": [
    "Thanks to Pranjal for the edge cases in the comments below. Suppose say there is no `http` or `https` in the url link. The function can now captures that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XLwwCDYHjfDa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Want to know more. Checkout  for additional information'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Want to know more. Checkout www.h2o.ai for additional information\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxwudahfjfDa"
   },
   "source": [
    "## **Removal of HTML Tags**\n",
    "\n",
    "One another common preprocessing technique that will come handy in multiple places is removal of html tags. This is especially useful, if we scrap the data from different websites. We might end up having html strings as part of our text.\n",
    "\n",
    "First, let us try to remove the HTML tags using regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDLvClCcjfDa"
   },
   "source": [
    "`Regex breakdown:`\n",
    "```Python\n",
    "'<.*?>'\n",
    "```\n",
    "* `<` and `>`: simply match the opening and closing brackets of HTML tags, e.g. \\<div>\n",
    "* `.*?`: This is the `non-greedy` or `lazy quantifier` *?, which matches any character (represented by `.` ) zero or more times, but it does so as few times as possible to make a valid match. In the context of HTML tags, this means it will match the shortest possible sequence of characters between the opening < and closing > tags.\n",
    "\n",
    "So, the entire regular expression `'<.*?>'` is used to match and capture the shortest possible HTML tag found in a text string. This is useful in cases where you want to extract or remove HTML tags from a text while preserving the shortest possible tag structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dGM79-tjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n"
     ]
    }
   ],
   "source": [
    "def remove_html(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags and content from the input text.\n",
    "\n",
    "    This function searches for HTML tags within the input text and removes them,\n",
    "    leaving only the plain text content.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing HTML tags to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with HTML tags and content removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_html(\"<p>This is <b>bold</b> text.</p>\")\n",
    "        \"This is bold text.\"\n",
    "    \"\"\"\n",
    "    html_pattern = r'<.*?>'\n",
    "    return re.sub(html_pattern, '', text)\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<SomeComponent/>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "print(remove_html('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkk2AxCgjfDb"
   },
   "source": [
    "We can also use `BeautifulSoup` package to get the text from HTML document in a more elegant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "2RHn33JFjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O \n",
      "  AutoML \n",
      "  Driverless AI\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text :str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags and content from the input text using BeautifulSoup.\n",
    "\n",
    "    This function utilizes the BeautifulSoup library to parse the input text as HTML\n",
    "    and then extracts and returns the plain text content, removing all HTML tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing HTML tags to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: A new string with HTML tags and content removed.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_html(\"<p>This is <b>bold</b> text.</p>\")\n",
    "        \"This is bold text.\"\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \").strip()\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGaEjvuBjfDb"
   },
   "source": [
    "**In your opinion which tool between regex or a parser is preferable and why? Write you thoughts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1llO37vjfDb"
   },
   "source": [
    "*Here you answer*\n",
    "\n",
    "The use of Parser is safer and more reliable espacially in the real-worf web data. \n",
    "Regex is more for simple and predictable HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISUr12uyjfDb"
   },
   "source": [
    "## **SpaCy**\n",
    "Similarly to NLTK, SpaCy is another popular NLP library with many features and pretrained models. It's extremely optimized and user-friendly.\n",
    "\n",
    "Sometimes less flexible if compared to NLTK (which provides a wide range of algorithms) it has comparable performance, or slightly better in tokenization and POS-tagging, underperforming in sentence tokenization.\n",
    "\n",
    "While NLTK can be seen as a large toolbox for NLP, SpaCy offers more ready-to-use solutions for production. Amoung the other funcionalities, there are:\n",
    "- Tokenization\n",
    "- Part-of-speech (POS) tagging\n",
    "- Dependency Parsing\n",
    "- Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Sentence Boundary Detection\n",
    "- Text Classification\n",
    "- etc\n",
    "\n",
    "It is worth it to that a look at it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znp3XdnPjfDb"
   },
   "source": [
    "Let's import the library and download a pretrained model for the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "DQYnSIYmjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------------------------- -------------- 8.1/12.8 MB 47.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 44.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpJIKGMdjfDb"
   },
   "source": [
    "The SpaCy's Language model (named \"nlp\" in this code) contains all the components and data needed for the analysis. Calling it on a text string will return a Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BWCJ-KIZjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\tPROPN\tnsubj\n",
      "is\tAUX\taux\n",
      "looking\tVERB\tROOT\n",
      "at\tADP\tprep\n",
      "buying\tVERB\tpcomp\n",
      "U.K.\tPROPN\tnsubj\n",
      "startup\tVERB\tccomp\n",
      "for\tADP\tprep\n",
      "$\tSYM\tquantmod\n",
      "1\tNUM\tcompound\n",
      "billion\tNUM\tpobj\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # pretrained model\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print('{0}\\t{1}\\t{2}'.format(token.text, token.pos_, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN7rerKsjfDb"
   },
   "source": [
    "As we have seen the first information we can get is the tokenization of the sequence with the corresponding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "l8iEBxaIjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "# tokenize the sentence collecting the tokens with a list comprehension\n",
    "# print the list of tokens\n",
    "\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Gvbmop5jfDb"
   },
   "source": [
    "Tokens contain many attributes like the corresponding lemma, pos tag, syntax dependency, word shape (e.g. capitalization, punctuation, digits), is alpha, is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "RN5C1jWijfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\t\tApple\t\tPROPN\t\tNNP\t\tnsubj\t\tXxxxx\t\tTrue\t\tFalse\n",
      "is\t\tbe\t\tAUX\t\tVBZ\t\taux\t\txx\t\tTrue\t\tTrue\n",
      "looking\t\tlook\t\tVERB\t\tVBG\t\tROOT\t\txxxx\t\tTrue\t\tFalse\n",
      "at\t\tat\t\tADP\t\tIN\t\tprep\t\txx\t\tTrue\t\tTrue\n",
      "buying\t\tbuy\t\tVERB\t\tVBG\t\tpcomp\t\txxxx\t\tTrue\t\tFalse\n",
      "U.K.\t\tU.K.\t\tPROPN\t\tNNP\t\tnsubj\t\tX.X.\t\tFalse\t\tFalse\n",
      "startup\t\tstartup\t\tVERB\t\tVBD\t\tccomp\t\txxxx\t\tTrue\t\tFalse\n",
      "for\t\tfor\t\tADP\t\tIN\t\tprep\t\txxx\t\tTrue\t\tTrue\n",
      "$\t\t$\t\tSYM\t\t$\t\tquantmod\t\t$\t\tFalse\t\tFalse\n",
      "1\t\t1\t\tNUM\t\tCD\t\tcompound\t\td\t\tFalse\t\tFalse\n",
      "billion\t\tbillion\t\tNUM\t\tCD\t\tpobj\t\txxxx\t\tTrue\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    string = '\\t\\t'.join([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop)])\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "p7pDJ8zpjfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "# remove stopwords using the information given by SpaCy\n",
    "# return the result as a list of tokens\n",
    "filtered = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "\n",
    "print(filtered)\n",
    "assert(filtered == ['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5ev-bg6jfDb"
   },
   "source": [
    "SpaCy also provides a Named Entity Recognition (NER) engine recognizing categories such as person, country, organization etc.\n",
    "\n",
    "Instead of iterating over the tokens, we rely on entities (ents) in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "PW1fX4s6jfDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO-7ltWLjfDc"
   },
   "source": [
    "To conclude let's visualize the dependecy tree instead of printing the result on the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "sH5SqTirjfDc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\douae\\Anaconda\\Lib\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"39d3ad1766784255a1054b6ca7265bc9-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39d3ad1766784255a1054b6ca7265bc9-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39d3ad1766784255a1054b6ca7265bc9-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.serve(doc, style=\"dep\", auto_select_port=True)\n",
    "\n",
    "# keep in mind to stop the execution after running this cell: it's a service running on your PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFJKmXoPjfDc"
   },
   "source": [
    "## **Subword Tokenization**\n",
    "\n",
    "What we have seen so far was a traditional pre-processing pipeline still relevant in some tasks, however nowadays the standard approach for NLP rely on huge neural models that can perform feature extraction by themself, without human intervention. In this context all the information are needed for the model: punctuation, emojis, stopwords and even case sensitive words.\n",
    "\n",
    "The only pre-processing steps required deal with data cleaning and sanitization, as well as tokenization. Pre-trained models usually have their own pre-trained tokenizer and sometimes it could be a good idea to pre-tokenize the input text, only once at the beginning, and work with sequences of tokens instead of strings.\n",
    "\n",
    "Let's use therefore a modern subword tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\douae\\anaconda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\douae\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\douae\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 40.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 40.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 561.5/561.5 kB 17.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 53.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/4 [safetensors]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ---------------------------------------- 4/4 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.34.4 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xs3KyBlAjfDc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\") # import the pre-trained tokenizer from BERT, a model we will see in the last lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB-MAjFRjfDc"
   },
   "source": [
    "We are now using the HugginFace ðŸ¤— library and in particular its tokenizers, with the tokenize() method we can directly see the effect of such tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nBXmpxV1jfDc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert',\n",
       " 'is',\n",
       " 'an',\n",
       " 'en',\n",
       " '##code',\n",
       " '##r',\n",
       " '-',\n",
       " 'only',\n",
       " 'transform',\n",
       " '##er',\n",
       " 'architecture',\n",
       " '!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"BERT is an encoder-only transformer architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doSwHUHFjfDc"
   },
   "source": [
    "Being it a subword tokenizer, words that do not appear in the (learned) vocabulary are splitted in subtokens, here identified with '##' at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K-tnloPajfDc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transform', '##er']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mjnckR3GjfDd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing', 'the', 'token', '##izer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"testing the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHgkvB56jfDd"
   },
   "source": [
    "Keep in mind, though, that in this context the goal is to prepare the data for ML analysis using neural models, subtokens per se are not really usefull for humans but\n",
    "Neural networks can only understand numbers, not string. As we have seen during the lecture the tokenizer will provide a numerical representation by converting tokens to word indeces (the corresponding index of the token in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v0yA_4sAjfDd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5604, 1996, 19204, 17629, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('testing the tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cKawGa7jfDd"
   },
   "source": [
    "You might have notices that compared to the tokenizer() method we have now 6 tokens instead of 4, converting the ids back to string can help us understanding what is goind on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UN7wBhhpjfDd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] testing the tokenizer [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('testing the tokenizer')\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5220xoKjfDd"
   },
   "source": [
    "BERT's tokenizer automatically added two special tokens at the begging and ending of the sequence, respectively [CLS] and [SEP] or 101 and 102 in numberical values.\n",
    "\n",
    "These tokens are used to delimit sentences and represent the entire sequence for tasks such as classification as we will see more in detail later on...\n",
    "\n",
    "convert_ids_to_tokens, and the dual method convert_tokens_to_ids, can perform this conversion between tokens and ids and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ks1NnWPwjfDd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(101))\n",
    "print(tokenizer.convert_ids_to_tokens(1996))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTsvabbOjfDd"
   },
   "source": [
    "Each word can therefore be splitted into one or more subtokens, depending on the tokenizer. In NLP \"fertility\" measures the average number of subwords produced per word by a tokenizer.\n",
    "\n",
    "Let's compute the fertility on the brown corpus: a pretokenized collection of texts already available on NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "70aC51xNjfDd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election']\n",
      "Number of words: 1161192\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('brown')\n",
    "brown = nltk.corpus.brown\n",
    "words = brown.words()\n",
    "\n",
    "print(words[:14])\n",
    "print('Number of words: ' + str(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kURJfzc3jfDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.128585970278817\n"
     ]
    }
   ],
   "source": [
    "# Here your code\n",
    "# compute the fertility on the brown corpus using BERT's pre-trained tokenizer\n",
    "tokenized_words = [tokenizer.tokenize(word) for word in words]\n",
    "num_original_words = len(words)\n",
    "num_tokenized_subwords = sum(len(subwords) for subwords in tokenized_words)\n",
    "fertility = num_tokenized_subwords / num_original_words \n",
    "print(fertility)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqRC8nQijfDe"
   },
   "source": [
    "**We computed the fertility on the Brown corpus (English texts) using a tokenizer trained for English, what if we used instead a multilingual model or a tokenizer trained in another language? Based on you understanding of the algorithm, what do you expect and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO6t4fQxjfDe"
   },
   "source": [
    "*Here you answer*\n",
    "\n",
    "Using a Non-English or multilingual  tokenize on English texts incease fertility, because more words are broken into subword tokens due to vocabulary mumatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfki6VGgjfDf"
   },
   "source": [
    "## +++ End of the mandatory section +++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUu6cfC4jfDf"
   },
   "source": [
    "## **Advance only**\n",
    "\n",
    "To get to the `advanced` grade try to put together what you have learned into a script and process the examples provided in `spam.tsv`. The final goal is to discriminate between spam and not spam using a random forest classifier. Adapt therefore the pre-processing pipeline to this need (e.g. removing or non removing stopwords? Punctuation? Stemming or Lemmatization? Other things to clean?). Please justify and comment on these decisions. Feel free to add as many steps as you feel necessary, as long as you justify the choice.\n",
    "\n",
    "Complete the `spam.py` script and try to run the classification process. How your decisions impacted the performance (pay attention to the F1-score)? Try different strategies and report what you have found, what are your explanations?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
