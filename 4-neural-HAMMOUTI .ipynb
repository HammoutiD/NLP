{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qnBJ_yLrxu5"
   },
   "source": [
    "*HAMMOUTI Douae*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-o65QYYrxu7"
   },
   "source": [
    "For this notebook a GPU environment is recomended, you could use the Nvidia T4 GPU provided by the Google Colab free plan (but keep in mind terms of service and limitations).\n",
    "\n",
    "## Transformers\n",
    "\n",
    "In this exercise session, we will implement a small Transformer model, encoder-only, for a classification task. For pedagogical reasons we will implement the encoder almost from stratch, learning how the components are integrated together. Then, in the second part, we will move to the Huggingface library using pre-trained models and applying a more standard pipeline: take a model pre-trained on a large scale dataset and finetune it on a specific task, updating only a smaller part of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\douae\\anaconda\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\douae\\anaconda\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EbXFfUDtrxu7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "cV87Vu28sYoD",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\douae\\anaconda\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\douae\\anaconda\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\douae\\anaconda\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\douae\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\douae\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\douae\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\douae\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\douae\\anaconda\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\douae\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHm3Hj5Dbg0n"
   },
   "source": [
    "For this tutorial we will use the Large Movie Review Dataset for binary sentiment classification with 50k annotated movie reviews. The dataset, available on Huggingface Hub, is splitted in `train` (25k), `test` (25k) and additional unlabeled data as well (`unsupervised`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "5ZfwQZCssIkY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKdfcyQFbg0o"
   },
   "source": [
    "For our purposes this dataset is even too big, therefore we will use on a small sample with 5k reviews taken from the original training set\n",
    "\n",
    "For every entry in the dataset we have the raw text and the sentiment label (negative=0, positive=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "HZXlWRM-su-A",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>Dumb is as dumb does, in this thoroughly unint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>I dug out from my garage some old musicals and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>After watching this movie I was honestly disap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640</th>\n",
       "      <td>This movie was nominated for best picture but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "6868   Dumb is as dumb does, in this thoroughly unint...          0\n",
       "24016  I dug out from my garage some old musicals and...          1\n",
       "9668   After watching this movie I was honestly disap...          0\n",
       "13640  This movie was nominated for best picture but ...          1\n",
       "14018  Just like Al Gore shook us up with his painful...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_DF = pd.DataFrame({'review':ds['train']['text'], 'sentiment':ds['train']['label']})\n",
    "IMDB_DF = IMDB_DF.sample(5000, random_state=42)\n",
    "IMDB_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIyTeX98bg0p"
   },
   "source": [
    "Let's see an example where it is possible to appreciate the presence of html tags in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ms-bF3QZrxu9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (Polish infos were even misguiding that they had the choice between being killed by own artillery or German guns, they even translated the title wrong as \"misplaced battalion\"). This have tickled the right spot and I bought the movie.<br /><br />The disappointment started when I realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies... Then I began to think \"hey, this story can\\'t be that simple... I bet this clever officer will find some cunning way to save what left of his troops\". Well, he didn\\'t, they were just sitting and waiting for something to happen. And so was I.<br /><br />The story was based on real events of World War I, so the writers couldn\\'t make much use of their imagination, but even thought I found this movie really unchallenging and even a little bit boring. And as I wrote in the first place - it isn\\'t fault of actors, writers or director - their marketing people have raised my expectations high above the level that this movie could cope with.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_DF['review'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEkW0Moubg0p"
   },
   "source": [
    "Before analysing the data therefore we need to perform some preprocessing, in particular lowercasing the text, HTML tags and URLs removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eUiFjlJorxu9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\douae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize as tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def imdb_preprocess(review: str) -> str:\n",
    "    '''\n",
    "    args:\n",
    "        review: the string of text containing the movie review\n",
    "    output:\n",
    "        the preprocessed string of text\n",
    "    '''\n",
    "\n",
    "    # Your code here\n",
    "    # remove HTML tags and URLs\n",
    "    # lowercase\n",
    "    # put your results into a variable named 'lowercased'\n",
    "    review = re.sub(r'<.*?>', ' ', review)\n",
    "    review = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', review, flags=re.MULTILINE)\n",
    "    lowercased = review.lower() \n",
    "    \n",
    "\n",
    "    # remove numbers and symbols\n",
    "    tokenized_words = tokenizer(lowercased)\n",
    "    words_alpha = [''.join(this_char for this_char in this_string if (this_char in string.ascii_lowercase)) for this_string in tokenized_words]\n",
    "    words_alpha = list(filter(None, words_alpha))\n",
    "    text = ' '.join(words_alpha)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c7lrYN3zrxu9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After watching this movie I was honestly disappointed - not because of the actors, story or directing - I was disappointed by this film advertisements.<br /><br />The trailers were suggesting that the battalion \"have chosen the third way out\" other than surrender or die (Polish infos were even misguiding that they had the choice between being killed by own artillery or German guns, they even translated the title wrong as \"misplaced battalion\"). This have tickled the right spot and I bought the movie.<br /><br />The disappointment started when I realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies... Then I began to think \"hey, this story can't be that simple... I bet this clever officer will find some cunning way to save what left of his troops\". Well, he didn't, they were just sitting and waiting for something to happen. And so was I.<br /><br />The story was based on real events of World War I, so the writers couldn't make much use of their imagination, but even thought I found this movie really unchallenging and even a little bit boring. And as I wrote in the first place - it isn't fault of actors, writers or director - their marketing people have raised my expectations high above the level that this movie could cope with.\n",
      "\n",
      "after watching this movie i was honestly disappointed not because of the actors story or directing i was disappointed by this film advertisements the trailers were suggesting that the battalion have chosen the third way out other than surrender or die polish infos were even misguiding that they had the choice between being killed by own artillery or german guns they even translated the title wrong as misplaced battalion this have tickled the right spot and i bought the movie the disappointment started when i realized that the third way is to just sit down and count dead bodies followed by sitting down and counting dead bodies then i began to think hey this story ca nt be that simple i bet this clever officer will find some cunning way to save what left of his troops well he did nt they were just sitting and waiting for something to happen and so was i the story was based on real events of world war i so the writers could nt make much use of their imagination but even thought i found this movie really unchallenging and even a little bit boring and as i wrote in the first place it is nt fault of actors writers or director their marketing people have raised my expectations high above the level that this movie could cope with\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# apply the imdb_preprocess function to the dataframe creating a new column named 'text'\n",
    "IMDB_DF['text'] = IMDB_DF['review'].apply(imdb_preprocess)\n",
    "\n",
    "# print the 3rd review and its corresponding preprocessed text  \n",
    "\n",
    "\n",
    "print(IMDB_DF.iloc[2]['review']+'\\n\\n'+IMDB_DF.iloc[2]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOEjKeaayV2D"
   },
   "source": [
    "Now, since we are training everything from scratch with a custom design, we need to implement the tokenizer too, and before that define the vocabulary.\n",
    "\n",
    "The vocabulary is composed by the words we met during the training with additional \"special tokens\", in particular \\<cls\\> (Classify Token),  \\<eos\\> (End Of Sentence) and  \\<pad\\> (Padding).\n",
    "\n",
    "The CLS will define the beginning of the sequence and could be used for classification tasks, being it a representation of the entire sequence. The EOS tag instead will delimit the end of a sentence. PAD represents the padded tokens needed to stack multiple tensors in the batch.\n",
    "\n",
    "With the `token2index`and `index2token`dictionaries we can perform the convertion between term and vocabulary index and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vHEI-89Crxu-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:05<00:00, 919.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 43077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CLS_token = '<cls>'\n",
    "EOS_token = '<eos>'\n",
    "PAD_token = '<pad>'\n",
    "CLS_index = 1\n",
    "EOS_index = 2\n",
    "PAD_index = 0\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.token2index = {'<pad>':0, '<cls>':1, '<eos>':2}\n",
    "        self.token2count = {}\n",
    "        self.index2token = {0: '<pad>', 1: '<cls>', 2: '<eos>'}\n",
    "        self.vocab_size = 3  # PAD, CLS and EOS\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def add_sentence(self, sentence, tokenizer):\n",
    "      for token in tokenizer(sentence):\n",
    "          self.add_word(token)\n",
    "\n",
    "    def add_word(self, token):\n",
    "      if token not in self.token2index:\n",
    "          self.token2index[token] = self.vocab_size\n",
    "          self.token2count[token] = 1\n",
    "          self.index2token[self.vocab_size] = token\n",
    "          self.vocab_size += 1\n",
    "      else:\n",
    "          self.token2count[token] += 1\n",
    "\n",
    "vocab = Vocab()\n",
    "\n",
    "for i, tab in tqdm(IMDB_DF.iterrows(), total=len(IMDB_DF)):\n",
    "  vocab.add_sentence(tab['text'], tokenizer)\n",
    "\n",
    "print('Vocabulary length: ' + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kGqH-3SyV2D"
   },
   "source": [
    "We should have 48959 words in our vocabulary.\n",
    "\n",
    "Let's see a small example, converting 'hello' to an index and the way back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "04ZQPvr9bg0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4227\n",
      "region\n"
     ]
    }
   ],
   "source": [
    "print(vocab.token2index['hello'])\n",
    "print(vocab.index2token[4295])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcf07jJ4yV2E"
   },
   "source": [
    "Because of the positional embedding in the Transformer architecture, the model can only accept in input a sequence limited in length. During the tokenization process we must keep this in mind, truncating the sentence if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "io28rn9Arxu-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def tokenize(text, tokenizer, vocab) -> list[int]:\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    token_ids = [\n",
    "        vocab.token2index[token] if token in vocab.token2index else vocab.token2index['<pad>']\n",
    "        for token in tokens\n",
    "    ]\n",
    "    token_ids = token_ids[:MAX_LEN - 2]\n",
    "    token_ids = [vocab.token2index['<cls>']] + token_ids + [vocab.token2index['<eos>']]\n",
    "    token_ids = token_ids[:MAX_LEN - 2]\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "# ✅ Tests\n",
    "assert(\n",
    "    tokenize(imdb_preprocess(IMDB_DF.iloc[0]['text']), tokenizer, vocab)[1:11]\n",
    "    == [3, 4, 5, 3, 6, 7, 8, 9, 10, 11]\n",
    ")\n",
    "assert(len(tokenize(imdb_preprocess('hello ' * 300), tokenizer, vocab)) == 254)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JLfQ8qqyV2E"
   },
   "source": [
    "And now we create a custom Pytorch Dataset returning a tuple composed by the input sequence and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lZxeC8vwrxu-"
   },
   "outputs": [],
   "source": [
    "class IMDBReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, vocab):\n",
    "        self.data = dataframe['text']\n",
    "        self.labels = dataframe['sentiment']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = tokenize(self.data.iloc[idx], tokenizer, vocab)\n",
    "        y = self.labels.iloc[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9Dpxue3bg0q"
   },
   "source": [
    "now we instantiate a new IMDBReviewDataset object and split the dataset into 'train_dataset', 'val_dataset' and 'test_dataset' with 80% - 10% - 10% ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aZl4Nf7Hbg0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_dataset:  4000\n",
      "Size of val_dataset:  500\n",
      "Size of test_dataset:  500\n",
      "\n",
      "[1, 7139, 21, 23, 1924, 41791, 105, 92, 481, 482, 86, 1619, 489, 71, 613, 5680, 921, 100, 1526, 23, 16536, 4588, 41, 32639, 1651, 110, 4737, 949, 197, 32, 5912, 166, 2417, 41, 921, 100, 1478, 33, 1189, 1178, 667, 193, 126, 2337, 7, 41792, 18812, 33, 3089, 952, 3828, 2135, 86, 36, 226, 79, 535, 36, 8, 353, 126, 6, 241, 195, 23, 1968, 21, 1896, 369, 921, 100, 4, 33, 197, 86, 673, 694, 188, 41, 5680, 498, 2217, 96, 481, 482, 69, 33, 746, 1140, 86, 1149, 92, 1369, 27751, 4, 7821, 33, 793, 143, 41793, 33, 2442, 4, 6833, 41794, 86, 323, 69, 7063, 716, 6069, 7432, 33, 273, 2531, 4, 23, 41795, 794, 510, 34762, 4402, 209, 23, 41796, 27751, 86, 23, 41797, 1651, 86, 33, 22976, 2074, 2531, 753, 158, 380, 41, 33, 273, 2531, 33, 15526, 3316, 5, 77, 5, 4529, 41798, 86, 23, 339, 4801, 12543, 14516, 143, 13091, 123, 444, 1053, 142, 3387, 83, 2961, 6159, 86, 673, 86, 1053, 323, 69, 29, 692, 2136, 1208, 1434, 33, 5234, 2]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = IMDBReviewDataset(IMDB_DF, tokenizer, vocab)\n",
    "train_dataset, val_dataset, test_dataset = random_split(imdb_dataset, [0.8, 0.1, 0.1], generator=torch.Generator())\n",
    "\n",
    "print('Size of train_dataset: ', len(train_dataset))\n",
    "print('Size of val_dataset: ', len(val_dataset))\n",
    "print('Size of test_dataset: ', len(test_dataset))\n",
    "print()\n",
    "x, y = train_dataset[0]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67gmuyW3bg0q"
   },
   "source": [
    "It's time now to add the special tokens to the sequence, CLS at the beginning and EOS at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mjIMn-R8bg0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before  [5, 6, 7]\n",
      "After:  tensor([1., 5., 6., 7., 2.])\n"
     ]
    }
   ],
   "source": [
    "tokens = [5,6,7]\n",
    "# Your code here\n",
    "# using this list as an example\n",
    "# concatenate CLS, tokens and EOS into a single torch tensor named 'input'\n",
    "input = torch.Tensor([CLS_index] + tokens + [EOS_index])    \n",
    "\n",
    "\n",
    "print('Before ',tokens)\n",
    "print('After: ',input)\n",
    "assert(torch.equal(input, torch.Tensor([1,5,6,7,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyMm0HbXyV2E"
   },
   "source": [
    "Finally we can create the batches used for the training, the data collator used by the DataLoader will provide us a list of samples to form a batch, given the desired batch size. We just need to implement the callable function to process the batch. To be able to stack multiple tensors together some padding is required, and consequently a padding mask to indicate on which token we should compute the attention. Pads are in fact excluded from the attention computation, whereas CLS and EOS are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9RdcbjiKrxu-"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    '''\n",
    "    args:\n",
    "        data_batch: list of N samples, with N being the batch size\n",
    "    outputs:\n",
    "        input_ids: tensor with shape N,L containing the token ids, with L the sequence length\n",
    "        labels: tensor with shape N,1 contaning the labels\n",
    "        mask: tensor with shape N,L representing the padding mask, indicating if the corresponding token is padding (mask == True) or not\n",
    "    '''\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    for tokens, label in data_batch:\n",
    "        # Add <cls> and <eos> tokens and convert to LongTensor\n",
    "        input = torch.tensor(tokens, dtype=torch.long)\n",
    "        label32 = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        input_ids.append(input)\n",
    "        labels.append(label32)\n",
    "\n",
    "    # Pad all sequences to same length\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=PAD_index)\n",
    "    labels = torch.stack(labels).unsqueeze(1)\n",
    "\n",
    "    # Create a mask for padding positions\n",
    "    mask = input_ids == PAD_index\n",
    "\n",
    "    return input_ids, labels, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC8eoZlWyV2E"
   },
   "source": [
    "Let's test the function before moving on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JAWd_goobg0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[5, 6, 7, 8],\n",
      "        [4, 9, 0, 0]])\n",
      "labels:  tensor([[0.],\n",
      "        [1.]])\n",
      "mask:  tensor([[False, False, False, False],\n",
      "        [False, False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "# example of batch containing sequences with different lengths and the corresponding labels\n",
    "example = [\n",
    "    ([5,6,7,8], 0),\n",
    "    ([4,9], 1)\n",
    "]\n",
    "\n",
    "input_ids, labels, mask = generate_batch(example)\n",
    "print('input_ids: ',input_ids)\n",
    "print('labels: ',labels)\n",
    "print('mask: ',mask)\n",
    "\n",
    "assert(torch.equal(labels, torch.Tensor([[0.],[1.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h3_uGityV2F"
   },
   "source": [
    "We expect to see tensors with the same dimensions and 0 pad tokens if needed (0 is the corresponding value in our vocabulary for the PAD token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkEn8kzPyV2F"
   },
   "source": [
    "We now have everything we need to create the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "CjSAdvGbrxu-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=generate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv5emHpUyV2F"
   },
   "source": [
    "For convenience, and taking inspiration from good pratices, we could group all the network hyper-parameters into a single class and then use this to instantiate our model.\n",
    "\n",
    "We will also use CUDA (if available) to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "iuz0U0xJ7BcJ"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    embd_pdrop = 0.2\n",
    "    resid_pdrop = 0.2\n",
    "    attn_pdrop = 0.2\n",
    "    head_pdrop = 0.2\n",
    "    n_layer = 6\n",
    "    n_head = 8\n",
    "    n_embd = 512\n",
    "    max_len = MAX_LEN\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        setattr(self, 'vocab_size', vocab_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = Config(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hQ1hV3vyV2F"
   },
   "source": [
    "It's time now to implement the attention block at the heart of the Transformer architecture. In the `__init__()` function we initialize all the components needed, in particulare the multi head attention. Two other methods (`_sa_block` and `_ff_block`) will deal with self attention and multi layer perceptron.\n",
    "\n",
    "More in detail, the block is composed by:\n",
    "\n",
    "1. the self-attention, implemented with PyTorch's `MultiheadAttention`, residual connection and layer normalization\n",
    "2. a feed forward network with residual connection and layer normalization\n",
    "\n",
    "We will also use dropout between the different components.\n",
    "\n",
    "Refer to the official documentation at [https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) to complete this exercise.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/336224014/figure/fig1/AS:809717848891393@1570063187530/The-structure-of-a-Transformer-Block.ppm\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TM8Vsyw27F9G"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.norm1 = nn.LayerNorm(config.n_embd)\n",
    "      self.norm2 = nn.LayerNorm(config.n_embd)\n",
    "      self.attn = nn.MultiheadAttention(config.n_embd, config.n_head, batch_first=True)\n",
    "      self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "      self.linear1 = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "      self.linear_drop = nn.Dropout(config.resid_pdrop)\n",
    "      self.activation = nn.GELU()\n",
    "      self.linear2 = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "      self.linear_drop2 = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x:Tensor, msk:Tensor) -> Tensor:\n",
    "      x = self.norm1(x + self._sa_block(x, msk))\n",
    "      x = self.norm2(x + self._ff_block(x))\n",
    "      return x\n",
    "\n",
    "    def _sa_block(self, x:Tensor, pad_mask:Tensor) -> Tensor:\n",
    "      # Your code here\n",
    "      # implement the self attention mechanism with the followin steps:\n",
    "      # 1. fill the arguments needed for the self.attn component (instance of nn.MultiheadAttention)\n",
    "      # 2. add dropout by calling self.attn_drop (instance of nn.Dropout)\n",
    "      attn_output, _ = self.attn(x, x, x, key_padding_mask=pad_mask)\n",
    "      attn_output = self.attn_drop(attn_output)\n",
    "      return attn_output\n",
    "\n",
    "    def _ff_block(self, x:Tensor) -> Tensor:\n",
    "      x = self.linear2(self.linear_drop(self.activation(self.linear1(x))))\n",
    "      return self.linear_drop2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrLgEFcfyV2F"
   },
   "source": [
    "**How are residual connections implemented in this code? In which lines?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE7eZhY9yV2F"
   },
   "source": [
    "*sidual connections (also called skip connections) mean that we add the input of a sublayer to its output before normalizing or passing further.*\n",
    "\n",
    "*residual connections are implemented on lines:\n",
    "\n",
    "      x = self.norm1(x + self._sa_block(x, msk))\n",
    "\n",
    "      x = self.norm2(x + self._ff_block(x))* \n",
    "\n",
    "\n",
    "*These make sure gradients flow better and that each block learns a refinement of its input rather than a completely new transformation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjye7AjQyV2F"
   },
   "source": [
    "**How is the feed forward network implemented? How many levels and with what dimensions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CbaPrywyV2K"
   },
   "source": [
    "*The feed-forward network is implemented in the _ff_block() method.\n",
    "\n",
    "It has 2 linear layers with dimensions 512 → 2048 → 512, using a GELU activation and dropout between them.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNlHZT3SyV2K"
   },
   "source": [
    "Until now we just converted words (strings of text) into indeces by looking at the corresponding token index in the vocabulary. Now, with this neural network, we should move to an higher space dimension, converting indeces into embeddings. You can think of this first layer as a lookup table, that will retrieve the corresponding (learnable) embedding given the token index.\n",
    "\n",
    "Look at the documentation for more info [https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OO3BwszUyV2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4337, -1.4741, -0.1260,  ...,  0.7362, -1.7791, -1.4085],\n",
      "        [ 0.0522,  0.0288, -0.3778,  ..., -0.4391, -1.7846, -1.7391],\n",
      "        [-0.7459, -0.2971,  0.8417,  ...,  1.4472,  0.4817, -0.9211],\n",
      "        ...,\n",
      "        [-0.4470, -0.1089, -1.0322,  ..., -3.1458,  0.4758, -0.1635],\n",
      "        [-1.5991,  0.5467,  0.3216,  ..., -0.3729, -0.2741,  1.4558],\n",
      "        [ 0.4416,  0.7609,  0.8655,  ..., -1.2192, -0.6141,  0.4698]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# instantiate an embedding lookup table with as many entries as our vocabulary and embeddings size equal to config.n_embd\n",
    "emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "embedding = emb(torch.Tensor([1,2,3,4,5,6,7,8,9,0]).long())\n",
    "print(embedding)\n",
    "print(embedding.shape)\n",
    "\n",
    "assert(embedding.shape == torch.Size([10, 512]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBHTNpGCyV2K"
   },
   "source": [
    "As we have seen, the attention mechanism is permutation equivariant, meaning that it does not take into account the order of the elements. In NLP, though, the word order is important. In this implementation we will use learnable positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Il-_LuyjyV2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0.2179, -1.1469, -0.5738,  ..., -1.7345, -0.9270, -2.5553],\n",
      "        [ 0.5729,  0.5867,  1.0441,  ...,  1.2253,  0.7463, -1.2076],\n",
      "        [-0.1359, -0.0544, -1.2718,  ..., -0.0752, -0.2488,  0.4072],\n",
      "        ...,\n",
      "        [-0.2880, -0.4251,  0.2210,  ..., -0.4282, -1.4895,  0.4274],\n",
      "        [-0.0512,  0.1810, -1.4433,  ...,  1.2636,  0.1479, -0.7344],\n",
      "        [ 0.0183, -0.0409,  1.3849,  ..., -0.4262, -0.3117, -1.6138]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([12, 512])\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# instantiate an embedding layer with config.max_len entries and same embeddings size as before\n",
    "pos_emb =   nn.Embedding(config.max_len, config.n_embd)\n",
    "\n",
    "# the input of this model consists of a sequence of ordered numbers spanning from 0 to the lenght of the sequence to encode\n",
    "input_pos = torch.arange(12, dtype=torch.long)\n",
    "print(input_pos)\n",
    "\n",
    "# the result of applying the positional embedding layer to the input is just another set of (learnable) embeddings\n",
    "embs = pos_emb(input_pos)\n",
    "print(embs)\n",
    "\n",
    "# with shape L,E where L is the sequence lenght and E the embedding size\n",
    "print(embs.shape)\n",
    "assert(embs.shape == torch.Size([12, 512]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1Iio8_YyV2K"
   },
   "source": [
    "With these components we can finally build the Transformer encoder to be used as classifier. To this aim we will add a \"classification head\" at the end, composed by a simple linear layer.\n",
    "\n",
    "The Transformer model is a sequence-to-sequence architecture, meaning that from a input sequence of length L it will return another sequence with the same length. For classification tasks, thought, it's sufficient to take the CLS embedding as representation for the entire sequence, and performing classification on that. \\(Other popular strategies could involve max pooling of average pooling over the sequence\\). In this case we will simply apply the classification head to the CLS embedding, that is the first embedding of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7GphHB3Prxu-"
   },
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "        # Your code here\n",
    "        # instantiate your embedding layer for converting input tokens into embedding vectors\n",
    "        self.emb = emb\n",
    "\n",
    "        # Your code here\n",
    "        # instantiate your positional embedding layer\n",
    "        self.pos_emb = pos_emb\n",
    "\n",
    "        self.norm_emb = nn.LayerNorm(config.n_embd)\n",
    "        self.drop_emb = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "        # repeat the Block module 'n_layer' times\n",
    "        self.blocks = nn.ModuleList(Block(config) for _ in range(config.n_layer))\n",
    "\n",
    "        # classification head\n",
    "        self.class_head = nn.Linear(config.n_embd, 1)\n",
    "        self.drop_head = nn.Dropout(config.head_pdrop)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x:Tensor, msk:Tensor) -> Tensor:\n",
    "        device = x.device\n",
    "\n",
    "        x = self.emb(x)  # N, L, E\n",
    "        position_ids = torch.arange(x.size(1), dtype=torch.long).unsqueeze(0).to(device)  # 1, L\n",
    "        pos_emb = self.pos_emb(position_ids)  # 1, L, E\n",
    "        x = x + pos_emb  # N, L, E\n",
    "        x = self.drop_emb(self.norm_emb(x))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, msk)\n",
    "\n",
    "        # Your code here\n",
    "        # given the output of the network x with shape (N,L,E)\n",
    "        # get the embedding corresponding to the CLS token (the first of the sequence)\n",
    "        # the output shape should be (N,E)\n",
    "        pooled = x[:, 0]\n",
    "\n",
    "        out = self.class_head(self.drop_head(pooled))\n",
    "        return out\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # weights initialization\n",
    "        self.emb.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.class_head.weight.data.uniform_(-0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUDeysLhyV2K"
   },
   "source": [
    "Now that the implementation is ready we can instantiate the model and load it into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "mW1zeypUrxu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (emb): Embedding(43077, 512)\n",
      "  (pos_emb): Embedding(256, 512)\n",
      "  (norm_emb): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop_emb): Dropout(p=0.2, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (attn_drop): Dropout(p=0.2, inplace=False)\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (linear_drop): Dropout(p=0.2, inplace=False)\n",
      "      (activation): GELU(approximate='none')\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (class_head): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (drop_head): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = EncoderClassifier(config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggoVpztqyV2K"
   },
   "source": [
    "For this simple training we will use the Adam optimizer and Binary Cross Entropy loss. In particular we rely on the nn.BCEWithLogitsLoss loss that includes a sigmoid function inside, meaning that - as the name suggests - we can use directly the logits (output) of the network as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xx8qEgU5rxu_"
   },
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoCuNpjRyV2L"
   },
   "source": [
    "Here our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sB_xiSxgrxu_"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion, device):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for inputs, labels, masks in tqdm(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, masks)\n",
    "\n",
    "        # Calculate the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the optimizer parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_epoch/len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS8tL8fSyV2L"
   },
   "source": [
    "For the evaluation phase, instead, we don't need to compute the gradients and perform backpropagation, we just need to run the model, compute and store the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "uVauRO6Grxu_"
   },
   "outputs": [],
   "source": [
    "def validate(model, testloader, criterion, device):\n",
    "    print('Validation')\n",
    "    loss_epoch = 0\n",
    "\n",
    "    # Your code here\n",
    "    # implement your validation loop keeping in mind to 1) put the model into evaluation mode and 2) disable gradient computation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, masks in tqdm(testloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch/len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8q-5oJYyV2M"
   },
   "source": [
    "Finally we can start the training, be aware that it could take a while.\n",
    "\n",
    "For this toy example we will limit the training to 4 epochs, monitoring the training and validation losses. If the validation loss is improving we will save the model on the disk for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "jy51yOnNrxu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [14:50<00:00, 14.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:23<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.810758268076276\n",
      "Validation loss: 0.694878563284874\n",
      "Saving best model till now...\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [17:05<00:00, 16.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:31<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7735601578439985\n",
      "Validation loss: 0.6915351301431656\n",
      "Saving best model till now...\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [18:38<00:00, 17.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:22<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7745105860725282\n",
      "Validation loss: 0.713542141020298\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [16:26<00:00, 15.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:21<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7340890916567\n",
      "Validation loss: 0.688492089509964\n",
      "Saving best model till now...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "train_loss, valid_loss = [], []\n",
    "least_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "\n",
    "    train_epoch_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_epoch_loss = validate(model, valid_loader, criterion, device)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    print(f\"Training loss: {train_epoch_loss}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}\")\n",
    "\n",
    "    # Save model\n",
    "    if valid_epoch_loss < least_loss:\n",
    "        least_loss = valid_epoch_loss\n",
    "        print(f\"Saving best model till now...\")\n",
    "        torch.save(model, 'model.pth')\n",
    "\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8f9tm7CyV2N"
   },
   "source": [
    "We should not expect great results from training such a small model for only 4 epochs on a toy dataset, but we can still visualize the loss evolution through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "hxm1RPeerxu_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [x for x in range(1, EPOCHS+1)]\n",
    "plt.plot(x, train_loss, label = \"train\")\n",
    "plt.plot(x, valid_loss, label = \"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faj3tAtXyV2N"
   },
   "source": [
    "Now that the training is over we can load the best performing model (on the validation set) we saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pK3EIROVrxu_"
   },
   "outputs": [],
   "source": [
    "trained_model = torch.load('model.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUijH2USyV2N"
   },
   "source": [
    "And evaluate it on the test set. In this case we will rely on the Scikit-learn library to compute the F1-score.\n",
    "\n",
    "This time, however, we need to create our predictions from the logits using a sigmoid function. For this purpose we can consider as positive class (1) probabilities that are stricly higher than 0.5 and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "r0hGIt3FIatI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:21<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.6942787244915962\n",
      "F1-score: 0.6329889898826631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test(model, testloader, criterion, device):\n",
    "    print('Testing')\n",
    "    model.eval()\n",
    "    loss_epoch = 0\n",
    "    f1_scores = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, masks in tqdm(testloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(inputs, masks)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "            # move tensors from GPU (is used) to CPU and flatten the representation\n",
    "            flatten_x = outputs.view(-1).cpu()\n",
    "            flatten_y = labels.long().view(-1).cpu()\n",
    "\n",
    "            # Your code here\n",
    "            # apply a sigmoid to flatten_x and make your predictions (1 iif prob > 0.5, 0 otherwise)\n",
    "            # convert the result into a long tensor (pytorch tensor with long type)\n",
    "            # Hint: use torch.sigmoid()\n",
    "            preds = (torch.sigmoid(flatten_x) > 0.5).long()\n",
    "\n",
    "            f1 = f1_score(flatten_y, preds, average='binary')\n",
    "            f1_scores += f1\n",
    "\n",
    "    return loss_epoch/len(testloader), f1_scores/len(testloader)\n",
    "\n",
    "test_loss, f1 = test(trained_model, test_loader, criterion, device)\n",
    "print('\\nTest loss: {}\\nF1-score: {}'.format(test_loss, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtq9yMRwyV2N"
   },
   "source": [
    "## Pretrained models with Huggingface\n",
    "\n",
    "We just learned how Transformer models work, this could be useful in the future if we want to modify, internally, the architecture, adapting or adding new components. We trained our model from scratch but we could have loaded pre-trained weights if we had them. Doing all this with native PyTorch might not be easy, though.\n",
    "\n",
    "That's why, for this second part, we will move to the Hugginface library (a de-facto standard when working with Transformers), using its implementations and pre-trained models already available on Hugginface Hub.\n",
    "We will perform the same classification task but in a much easier way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXv5aCCpyV2N"
   },
   "source": [
    "Let's download an already pre-processed version of the Large Movie Review Dataset taking only 5k samples from the train split.\n",
    "\n",
    "For this example we will use only two splits for training and validation respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "dzgZD8peZ8dA"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1488279a0be74eccbe663ca5ebea151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\douae\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\douae\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c11965ec764106aa7e1dc8351f937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18258f03fbb342fbb31b94a94b7f989c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b2ef88205744f09b35902b24e8acee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f2b683db9e4b0d9aebf670d4cb3202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c8a4c528f54060b21efed8c037b1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c804c8d1355345f89f4d53394f53cd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\", verification_mode='no_checks', split='train[0:5000]')\n",
    "imdb = imdb.train_test_split(test_size=0.1)\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLqsSXSbyV2O"
   },
   "source": [
    "Let's use a more advanced subword tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Zj9tbPivRp87"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5783543a15564dc9bf81efefd83f106f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\douae\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\douae\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975579c3dff14c3cb56dc2e20bc92332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13b9169ecc042f7a2d2f4087eb6b371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c8199be409477387a4d779f899f33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j4utOAJyV2O"
   },
   "source": [
    "**Do you remember any advantages in using a subword tokenizers? What are the main features and why we are using them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXlcFq0XyV2O"
   },
   "source": [
    "*Subword tokenizers are used because they handle unknown words by breaking them into smaller known pieces, reduce vocabulary size, and improve model generalization. They balance word- and character-level tokenization, making models more efficient and robust across different words and languages.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GO2Q9DyXyV2O"
   },
   "source": [
    "We need to rewrite our collator function by replacing the tokenizer. Notice that this new tokenizer can already do truncation and padding, working directly on a list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "G6b3_iESQdhL"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    batch = []\n",
    "    labels = []\n",
    "    for data in data_batch:\n",
    "        batch.append(data['text'])\n",
    "        labels.append(torch.tensor(data['label']).long())\n",
    "\n",
    "    batch = tokenizer(batch, truncation=True, padding='longest')\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    input_ids = torch.tensor(batch['input_ids'])\n",
    "    attention_mask = torch.tensor(batch['attention_mask'])\n",
    "    return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3yW_aRdyV2O"
   },
   "source": [
    "Let's create two data loaders for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "mGnoAbLkbc8L"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(imdb['train'], batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=generate_batch)\n",
    "valid_loader = DataLoader(imdb['test'], batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q8D_HozyV2O"
   },
   "source": [
    "And now we can load the pretrained model; we will use DistilBERT: a ligther version of BERT (fewer parameters) obtained by distilling it (through knowledge distillation). It will make our training faster while preserving good performances.\n",
    "\n",
    "Take a look at the documentation at [https://huggingface.co/docs/transformers/model_doc/distilbert](https://huggingface.co/docs/transformers/model_doc/distilbert).\n",
    "You will notice that we are using the `DistilBertForSequenceClassification` model, that is a `DistilBertModel` with a classification head on top. However only the base model is pretrained, the linear classification layers are just randomly initialized (see the warning that appears after loading the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "id": "7Ic6jj4dbhd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c981205d1e447ea30bbad5bec7ae62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yvc1dRPCyV2O"
   },
   "source": [
    "**Based on the printed output, what information we could get from the model? How many layers is it using? What is the internal embedding dimension? And the maximum sequence lenght?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys8Bw7VjyV2O"
   },
   "source": [
    "*From the printed model summary, we can see that:\n",
    "\n",
    "✅ Model type: DistilBERT (a smaller, faster version of BERT).\n",
    "\n",
    "✅ Number of Transformer layers: 6 ((0-5): 6 x TransformerBlock).\n",
    "\n",
    "✅ Embedding dimension: 768 (from Embedding(30522, 768, ...)).\n",
    "\n",
    "✅ Maximum sequence length: 512 (from position_embeddings: Embedding(512, 768)).\n",
    "\n",
    "So, the model uses 6 layers, 768 hidden dimensions, and supports sequences up to 512 tokens long.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYOVFzEQ585B"
   },
   "source": [
    "Unlike the first part of this notebook, when we trained a new model from scratch, this time we have a pre-trained model with all its benefits. In particular we can rely on previously trained weights that already have a good knowledge of the language and finetune the model on our downstream task (classification in this case).\n",
    "\n",
    "Moreover, to save computational resources and avoid overfitting, as well as preventing catastrophic forgetting of previously learned features, we can freeze some layers and train only those who are necessary.\n",
    "When deciding how much of a pre-trained model to fine-tune, there is no definitive answer as it depends on several factors. The similarity between the original task and the new task, the size and complexity of the pre-trained model, as well as the size and quality of the new data all play a role. For example, if the tasks are very similar, you may only need to fine-tune the last layer or the classifier head of the pre-trained model. However, if the tasks are very different or you have a lot of high-quality data for the new task, you can afford to fine-tune more layers or even the whole model.\n",
    "\n",
    "In this toy example we will freeze all the pre-trained weights from DistillBERT, training only the classification head (final MLP). In PyTorch is very easy to prevent gradient computation, indeed it is sufficient to set the `requires_grad` attribute of a network parameter to `False` to freeze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-jPV613585C"
   },
   "source": [
    "Let's visualize the list of parameters (network weights) of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xLuoaoKJ585C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([30522, 768])\tName: distilbert.embeddings.word_embeddings.weight\n",
      "Shape: torch.Size([512, 768])\tName: distilbert.embeddings.position_embeddings.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.embeddings.LayerNorm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.embeddings.LayerNorm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.0.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.0.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.0.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.0.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.0.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.1.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.1.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.1.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.1.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.1.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.2.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.2.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.2.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.2.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.2.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.3.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.3.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.3.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.3.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.3.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.4.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.4.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.4.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.4.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.4.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.5.attention.q_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.attention.q_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.5.attention.k_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.attention.k_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.5.attention.v_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.attention.v_lin.bias\n",
      "Shape: torch.Size([768, 768])\tName: distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.attention.out_lin.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.sa_layer_norm.bias\n",
      "Shape: torch.Size([3072, 768])\tName: distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "Shape: torch.Size([3072])\tName: distilbert.transformer.layer.5.ffn.lin1.bias\n",
      "Shape: torch.Size([768, 3072])\tName: distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.ffn.lin2.bias\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "Shape: torch.Size([768])\tName: distilbert.transformer.layer.5.output_layer_norm.bias\n",
      "Shape: torch.Size([768, 768])\tName: pre_classifier.weight\n",
      "Shape: torch.Size([768])\tName: pre_classifier.bias\n",
      "Shape: torch.Size([2, 768])\tName: classifier.weight\n",
      "Shape: torch.Size([2])\tName: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print('Shape: {}\\tName: {}'.format(param.shape, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6woGXMh-585C"
   },
   "source": [
    "We will freeze everything but the classification layers, i.e., `pre_classifier`and `classifier` linear layers (both weight and bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "O3bNOaftcrn3"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0mJVl6R585D"
   },
   "source": [
    "Let's now collect the parameters we want to train and that will be used by the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "eVtTNMcl585D"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# create a list named 'params' containing only the parameters we want to train\n",
    "# use list comprehension and rely on the attribute\n",
    "params = [param for param in model.parameters() if param.requires_grad] \n",
    "\n",
    "\n",
    "assert(all([x.requires_grad for x in params]) and len([x.requires_grad for x in params]) == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpKdHcL0585D"
   },
   "source": [
    "These parameters will be optimized by the AdamW optimizer based on the cross entropy loss. Please notice that this time the final layer of the network is a linear layer with 2 output features (our two classes), therefore we will rely the (log) softmax already implemented inside `nn.CrossEntropyLoss` to compute the probabilities and the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "8qsNiBoVda8E"
   },
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(params, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCxlm7Cz585D"
   },
   "source": [
    "Here our training and validation loops, depending on the function parameters, the Huggingface model may return different outputs (e.g., the attention weights, hidden states of the loss). We are only interested to the logits, i.e. the unnormalised predictions (output) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "wc9jK36Gdeum"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion, device):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for input_ids, att_mask, labels in tqdm(trainloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        att_mask = att_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(input_ids, att_mask)['logits']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_epoch/len(trainloader)\n",
    "\n",
    "def validate(model, testloader, criterion, device):\n",
    "    print('Validation')\n",
    "    model.eval()\n",
    "    loss_epoch = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, att_mask, labels in tqdm(testloader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            att_mask = att_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, att_mask)['logits']\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch/len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-jowpOq585E"
   },
   "source": [
    "Finally we can train (finetune) our model over 4 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "u5nhbyfddryl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [39:20<00:00, 33.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:01<00:00, 15.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4604246910189239\n",
      "Validation loss: 0.28752104565501213\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [32:15<00:00, 27.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:15<00:00, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.20419943437609875\n",
      "Validation loss: 0.11778188589960337\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [33:14<00:00, 28.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:03<00:00, 15.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0904833663412383\n",
      "Validation loss: 0.05330787319689989\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 4\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [30:41<00:00, 25.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:53<00:00, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.04569711709316348\n",
      "Validation loss: 0.028759789653122425\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "train_loss, valid_loss = [], []\n",
    "least_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "\n",
    "    train_epoch_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_epoch_loss = validate(model, valid_loader, criterion, device)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    print(f\"Training loss: {train_epoch_loss}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}\")\n",
    "\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b43RhIRJ585E"
   },
   "source": [
    "and visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "i9BjFilDylv5",
    "outputId": "62c491f8-772c-4582-cdb0-856fe229ed5a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [x for x in range(1, EPOCHS+1)]\n",
    "plt.plot(x, train_loss, label = \"train\")\n",
    "plt.plot(x, valid_loss, label = \"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP37TGJn585F"
   },
   "source": [
    "## +++ End of the mandatory section +++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HHBjJQS585F"
   },
   "source": [
    "## **Advance only**\n",
    "\n",
    "Jupyter notebook could be useful for prototyping and testing new ideas, in this tutorial we explored step-by-step how to build a modern NLP classifier. However, once the code is ready and tested, it is convenient to have a native python script ready to run on your enviroment (colab, a virtual machine on the cloud or a cluster).\n",
    "\n",
    "Try to collect and rewrite the classification pipeline with Huggingface into a single Python script, removing redundant parts and useless variables (sometimes we create new variables just for the explanation). Make the code easy to use and generalizable, for example it should be easy to change the pretrained model if needed for another experiment. Use command line arguments to set the hyper-parameters of the experiments (e.g. learning rate, epochs, batch size etc.). Feel free to experiment with your classifier but be careful of the computational limits you may encounter. Keep in mind that the goal of this lab was to introduce the classification task and familiarize with standard libraries, not to release a new state-of-the-art model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
